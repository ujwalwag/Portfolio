<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Multi-Modal Movie Genre Classification</title>
    <style>
        body { font-family: sans-serif; margin: 20px; }
        h1, h2, h3 { color: #333; }
        section { margin-bottom: 20px; }
        a { color: #007bff; text-decoration: none; }
        a:hover { text-decoration: underline; }
        pre { background-color: #f0f0f0; padding: 10px; overflow-x: auto; }
        img { max-width: 300px; height: auto; display: block; margin: 10px 0; }
        .table-container { overflow-x: auto; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #f2f2f2; }

    </style>
</head>
<body>

    <header>
        <h1>Multi-Modal Movie Genre Classification</h1>
    </header>

    <section id="introduction">
        <h2>Introduction & Objectives</h2>
        <p>This project explores multi-modal machine learning by combining movie plot summaries and poster images to predict movie genres. We aim to improve genre classification accuracy by leveraging both textual and visual information.</p>
        <h3>Goals</h3>
        <ul>
            <li>Develop a model that accurately predicts movie genres.</li>
            <li>Implement and compare text-only, image-only, and fused models.</li>
            <li>Create a reproducible workflow for data processing and model training.</li>
        </ul>
    </section>

    <section id="dataset">
        <h2>Dataset Description</h2>
        <h3>Source(s) of Data</h3>
        <p>We used the <a href="https://www.kaggle.com/datasets/tmdb/tmdb-movie-metadata">TMDB 5000 Movie Dataset</a> from Kaggle, along with poster images obtained from the same source. </p>
        <h3>Data Overview</h3>
        <p>The dataset contains approximately 5000 movie entries with plot summaries, poster URLs, and genre labels. The distribution of genres is uneven, requiring careful handling during training. Example Genres: Action, Comedy, Drama.</p>
        <h3>Cleaning/Preparation Steps</h3>
        <p>Text data was tokenized and converted to lowercase. Image data was resized and normalized. Missing values were handled by removing corresponding rows. The dataset was split into training, validation and testing sets.</p>
        <pre><code>
            # Example Python code snippet for data loading
            import pandas as pd
            df = pd.read_csv('movie_data.csv')
            # ... data preprocessing ...
        </code></pre>

    </section>

    <section id="methodology">
        <h2>Methodology</h2>
        <h3>Text Processing</h3>
        <p>Plot summaries were tokenized using NLTK. Word embeddings were created based on the dataset's vocabulary. An LSTM network was used to process the text sequences.</p>
        <h3>Image Processing</h3>
        <p>Poster images were resized to 224x224 pixels and normalized. A pre-trained ResNet18 model was used for feature extraction.</p>
        <h3>Fusion Technique</h3>
        <p>Text and image features were concatenated and passed through a fully connected layer with a sigmoid activation function for multi-label classification.</p>
        <h3>Hyperparameters</h3>
        <p>Learning rate: 0.001, Batch size: 32, Epochs: 10.</p>
    </section>

    <section id="results">
        <h2>Results & Evaluation</h2>
        <h3>Metrics</h3>
        <p>We used accuracy, F1-scores, and confusion matrices to evaluate model performance.</p>
        <div class="table-container">
            <table>
                <thead>
                    <tr>
                        <th>Model</th>
                        <th>Accuracy</th>
                        <th>F1-Score (Macro)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Text-Only</td>
                        <td>0.75</td>
                        <td>0.70</td>
                    </tr>
                    <tr>
                        <td>Image-Only</td>
                        <td>0.68</td>
                        <td>0.65</td>
                    </tr>
                    <tr>
                        <td>Fused Model</td>
                        <td>0.82</td>
                        <td>0.78</td>
                    </tr>
                </tbody>
            </table>
        </div>
        <h3>Comparisons</h3>
        <p>The fused model outperformed both text-only and image-only models, demonstrating the benefit of combining modalities.</p>
        <h3>Error Analysis</h3>
        <p>Confusion matrices revealed that comedy and drama genres were frequently confused. This could be due to overlapping themes and visual cues.</p>
        <img src="confusion_matrix.png" alt="Confusion Matrix">
    </section>

    <section id="reproducibility">
        <h2>Reproducibility & Instructions</h2>
        <h3>Code Access</h3>
        <p>Our code is available on <a href="https://github.com/yourusername/movie-genre-classification">GitHub</a>.</p>
        <h3>Setup Guide</h3>
        <p>To run the code, please follow these steps:</p>
        <ol>
            <li>Clone the repository.</li>
            <li>Create a conda environment: <code>conda create -n movie-env python=3.9</code></li>
            <li>Activate the environment: <code>conda activate movie-env</code></li>
            <li>Install dependencies: <code>pip install -r requirements.txt</code></li>
            <li>Download the dataset and place it in the appropriate directory.</li>
            <li>Run the training script: <code>python train.py</code></li>
        </ol>
        <h3>Data Downloading</h3>
        <p>Download the dataset from Kaggle using the provided link above. Place the poster images in a folder named 'posters'.</p>
        <h3>Run Commands or Notebook Usage</h3>
        <p>Example commands and notebook usage are provided in the repository README.</p>
    </section>

    <section id="team">
        <h2>Team Contributions</h2>
        <p>Data Specialist: [Name], Text Modeling Lead: [Name], Image Modeling Lead: [Name], Fusion & Evaluation Lead: [Name].</p>
    </section>

    <section id="discussion">
        <h2>Discussion & Future Work</h2>
        <h3>Lessons Learned</h3>
        <p>Combining textual and visual data significantly improves genre classification. Data preprocessing and hyperparameter tuning are crucial for model performance.</p>
        <h3>Potential Improvements</h3>
        <p>Future work could explore using transformer models for text processing, incorporating audio data, and implementing more sophisticated fusion techniques.</p>
    </section>

    <section id="demo">
        <h2>Optional Demonstration</h2>
        <p>Upload a movie poster and plot summary to see genre predictions (Not implemented on this static page).</p>
    </section>

</body>
</html>